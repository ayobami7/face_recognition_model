{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40520,"status":"ok","timestamp":1754895834445,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"3YJUz3grE1jt","outputId":"39585519-2ff6-43c1-e907-104502b29eee"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m604.2/611.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","inflect 7.5.0 requires typeguard\u003e=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mCollecting keras-facenet\n","  Downloading keras-facenet-0.3.2.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting mtcnn (from keras-facenet)\n","  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: joblib\u003e=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn-\u003ekeras-facenet) (1.5.1)\n","Collecting lz4\u003e=4.3.3 (from mtcnn-\u003ekeras-facenet)\n","  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: keras-facenet\n","  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-facenet: filename=keras_facenet-0.3.2-py3-none-any.whl size=10367 sha256=0ab9975b147dc3e16b3ebc382577b4926911af287117f542b026663127de94ac\n","  Stored in directory: /root/.cache/pip/wheels/99/94/dd/cb1a65a7440ba6d508bd24346c15af0b1d24ff8b1cdb1c9959\n","Successfully built keras-facenet\n","Installing collected packages: lz4, mtcnn, keras-facenet\n","Successfully installed keras-facenet-0.3.2 lz4-4.4.4 mtcnn-1.0.0\n"]}],"source":["!pip install -q tensorflow tensorflow-addons tensorflow-hub tensorflow-datasets\n","!pip install -q gdown matplotlib seaborn\n","!pip install keras-facenet\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, optimizers, losses, metrics, Model\n","import tensorflow_hub as hub\n","from keras_facenet import FaceNet\n","import pandas as pd\n","import numpy as np\n","import os\n","from pathlib import Path\n","import zipfile\n","import gdown\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive, files\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1466,"status":"ok","timestamp":1754896145023,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"TcEuppv89pwx","outputId":"2c950862-e39d-4086-b4aa-8c3d34b4f658"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["drive.mount('/content/drive')\n","DATASET_PATH = \"/content/drive/MyDrive/datasets/FairFace\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41361,"status":"ok","timestamp":1754896195976,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"yb7BNSQsAh1j","outputId":"c92cea7f-1068-488e-ffe4-c1cc8dc9e8ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Path exists.\n","Loading dataset from: /content/drive/MyDrive/datasets/FairFace\n","\n","Dataset structure verified:\n","Training CSV:   /content/drive/MyDrive/datasets/FairFace/train_labels.csv\n","Validation CSV: /content/drive/MyDrive/datasets/FairFace/val_labels.csv\n","Training images: /content/drive/MyDrive/datasets/FairFace/train (86744 files)\n","Validation images: /content/drive/MyDrive/datasets/FairFace/val (10974 files)\n"]}],"source":["\n","if os.path.exists(DATASET_PATH):\n","    print(\"Path exists.\")\n","else:\n","    print(\"Path does not exist.\")\n","\n","# Verify dataset structure\n","expected_files = [\n","    \"train_labels.csv\",\n","    \"val_labels.csv\",\n","    \"train/\",\n","    \"val/\"\n","]\n","\n","print(f\"Loading dataset from: {DATASET_PATH}\")\n","\n","# Check if all required files/folders exist\n","missing_files = []\n","for item in expected_files:\n","    if not os.path.exists(os.path.join(DATASET_PATH, item)):\n","        missing_files.append(item)\n","\n","if missing_files:\n","    raise FileNotFoundError(\n","        f\"Dataset incomplete. Missing: {missing_files}\\n\"\n","        f\"Expected structure:\\n\"\n","        f\"{DATASET_PATH}/\\n\"\n","        f\"├── train_labels.csv\\n\"\n","        f\"├── val_labels.csv\\n\"\n","        f\"├── train/ [contains images]\\n\"\n","        f\"└── val/   [contains images]\"\n","    )\n","\n","\n","# Set paths for data loading\n","TRAIN_CSV_PATH = os.path.join(DATASET_PATH, \"train_labels.csv\")\n","VAL_CSV_PATH = os.path.join(DATASET_PATH, \"val_labels.csv\")\n","TRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"train\")\n","VAL_IMG_DIR = os.path.join(DATASET_PATH, \"val\")\n","\n","print(\"\\nDataset structure verified:\")\n","print(f\"Training CSV:   {TRAIN_CSV_PATH}\")\n","print(f\"Validation CSV: {VAL_CSV_PATH}\")\n","print(f\"Training images: {TRAIN_IMG_DIR} ({len(os.listdir(TRAIN_IMG_DIR))} files)\")\n","print(f\"Validation images: {VAL_IMG_DIR} ({len(os.listdir(VAL_IMG_DIR))} files)\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1754896457884,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"WONN5P3SVJmf"},"outputs":[],"source":["class FairFaceDataProcessor:\n","    def __init__(self, csv_path, img_dir, img_size=160, sample_size=None, is_validation=False):\n","        self.img_size = img_size\n","        self.img_dir = img_dir\n","        self.is_validation = is_validation\n","\n","        # Load CSV data\n","        self.df = pd.read_csv(csv_path)\n","\n","        # Filter dataset to keep only valid age groups\n","        valid_age_groups = [\n","            '0-2', '3-9', '10-19', '20-29', '30-39',\n","            '40-49', '50-59', '60-69', 'more than 70'\n","        ]\n","        original_len = len(self.df)\n","        self.df = self.df[self.df['age'].isin(valid_age_groups)].reset_index(drop=True)\n","        print(f\"Removed {original_len - len(self.df)} rows with invalid age groups\")\n","\n","        # Sample data if specified\n","        if sample_size and sample_size \u003c len(self.df):\n","            self.df = self.df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n","            print(f\"Sampled {'validation' if is_validation else 'training'}: {len(self.df)} samples\")\n","\n","        # Initialize label encoders only once (use training data to fit)\n","        if not hasattr(self, 'age_encoder'):\n","            self.age_encoder = LabelEncoder()\n","            self.gender_encoder = LabelEncoder()\n","            self.race_encoder = LabelEncoder()\n","\n","            # Fit encoders on this dataset\n","            self.age_encoder.fit(self.df['age'])\n","            self.gender_encoder.fit(self.df['gender'])\n","            self.race_encoder.fit(self.df['race'])\n","\n","        # Encode labels\n","        self.df['age_encoded'] = self.age_encoder.transform(self.df['age'])\n","        # self.df['gender_encoded'] = self.gender_encoder.transform(self.df['gender'])\n","        self.df['gender_encoded'] = (self.df['gender'] == 'Male').astype(int)\n","        self.df['race_encoded'] = self.race_encoder.transform(self.df['race'])\n","\n","        self.num_classes = {\n","            'age': len(self.age_encoder.classes_),\n","            'gender': len(self.gender_encoder.classes_),\n","            'race': len(self.race_encoder.classes_)\n","        }\n","\n","        print(f\"Classes - Age: {self.num_classes['age']}, Gender: {self.num_classes['gender']}, Race: {self.num_classes['race']}\")\n","        if not is_validation:\n","            print(f\"Age groups: {list(self.age_encoder.classes_)}\")\n","            print(f\"Gender groups: {list(self.gender_encoder.classes_)}\")\n","            print(f\"Race groups: {list(self.race_encoder.classes_)}\")\n","\n","    def filter_missing_files(self):\n","        \"\"\"Filter out rows where image files don't exist\"\"\"\n","        print(f\"Original dataset size: {len(self.df)}\")\n","\n","        # Check which files exist\n","        existing_files = []\n","        for idx, row in self.df.iterrows():\n","            img_path = os.path.join(self.img_dir, row['file'])\n","            if os.path.exists(img_path):\n","                existing_files.append(idx)\n","\n","        # Filter dataframe to keep only existing files\n","        self.df = self.df.loc[existing_files].reset_index(drop=True)\n","\n","        print(f\"Filtered dataset size: {len(self.df)}\")\n","\n","        return self\n","\n","    def load_and_preprocess_image(self, image_path, augment=False):\n","        \"\"\"Load and preprocess single image\"\"\"\n","        try:\n","            # Check if file exists\n","            if not tf.io.gfile.exists(image_path):\n","                print(f\"Warning: File not found: {image_path}\")\n","                return tf.zeros([self.img_size, self.img_size, 3], dtype=tf.float32)\n","\n","            image = tf.io.read_file(image_path)\n","            image = tf.image.decode_image(image, channels=3)\n","            image = tf.image.resize(image, [self.img_size, self.img_size])\n","            image = tf.cast(image, tf.float32) / 255.0\n","\n","            if augment:\n","                # Data augmentation\n","                image = tf.image.random_flip_left_right(image)\n","                image = tf.image.random_brightness(image, 0.1)\n","                image = tf.image.random_contrast(image, 0.9, 1.1)\n","                image = tf.image.random_saturation(image, 0.9, 1.1)\n","                image = tf.image.random_hue(image, 0.05)\n","\n","            # Normalize using ImageNet statistics (for FaceNet compatibility)\n","            image = tf.image.per_image_standardization(image)\n","            return image\n","        except Exception as e:\n","            print(f\"Error loading image {image_path}: {e}\")\n","            # Return black image if file not found\n","            return tf.zeros([self.img_size, self.img_size, 3], dtype=tf.float32)\n","\n","    def create_dataset(self, batch_size=32, augment=False, shuffle=True):\n","        \"\"\"Create TensorFlow dataset\"\"\"\n","        def generator():\n","            indices = np.arange(len(self.df))\n","            if shuffle:\n","                np.random.shuffle(indices)\n","\n","            for idx in indices:\n","                row = self.df.iloc[idx]\n","\n","                img_path = os.path.join(self.img_dir, row['file'])\n","\n","                image = self.load_and_preprocess_image(img_path, augment)\n","\n","                yield (\n","                    image,\n","                    {\n","                        'age_output': tf.cast(row['age_encoded'], dtype=tf.int32),\n","                        'gender_output': tf.cast(row['gender_encoded'], dtype=tf.int32),\n","                        'race_output': tf.cast(row['race_encoded'], dtype=tf.int32)\n","                    }\n","                )\n","\n","        # Create dataset\n","        dataset = tf.data.Dataset.from_generator(\n","            generator,\n","            output_signature=(\n","                tf.TensorSpec(shape=(self.img_size, self.img_size, 3), dtype=tf.float32),\n","                {\n","                    'age_output': tf.TensorSpec(shape=(), dtype=tf.int32),\n","                    'gender_output': tf.TensorSpec(shape=(), dtype=tf.int32),\n","                    'race_output': tf.TensorSpec(shape=(), dtype=tf.int32)\n","                }\n","            )\n","        )\n","\n","        if shuffle:\n","            dataset = dataset.shuffle(buffer_size=1000)\n","\n","        dataset = dataset.batch(batch_size)\n","        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","        return dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1754896206318,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"1drJhus-PYHr"},"outputs":[],"source":["class FaceNetMultiTask(tf.keras.Model):\n","    def __init__(self, num_age_classes, num_gender_classes, num_race_classes,\n","                 freeze_backbone=False):\n","        super(FaceNetMultiTask, self).__init__()\n","\n","        self.backbone = FaceNet().model\n","\n","        # Freeze backbone\n","        if freeze_backbone:\n","            self.backbone.trainable = False\n","\n","        # # Classification heads\n","        # self.age_classifier = tf.keras.Sequential([\n","        #     layers.Dense(512, activation='relu'),\n","        #     layers.Dropout(0.7),\n","        #     layers.Dense(128, activation='relu'),\n","        #     layers.Dense(num_age_classes, activation='softmax', name='age_output')\n","        # ], name='age_head')\n","\n","        # self.gender_classifier = tf.keras.Sequential([\n","        #     layers.Dense(512, activation='relu'),\n","        #     layers.Dropout(0.7),\n","        #     layers.Dense(128, activation='relu'),\n","        #     layers.Dense(1, activation='sigmoid', name='gender_output')\n","        # ], name='gender_head')\n","\n","        # self.race_classifier = tf.keras.Sequential([\n","        #     layers.Dense(512, activation='relu'),\n","        #     layers.Dropout(0.7),\n","        #     layers.Dense(128, activation='relu'),\n","        #     layers.Dense(num_race_classes, activation='softmax', name='race_output')\n","        # ], name='race_head')\n","\n","        # Shared dense layer\n","        self.shared_dense = layers.Dense(512, activation='relu', name='shared_dense')\n","        self.shared_dropout = layers.Dropout(0.7, name='shared_dropout')\n","\n","        # Task-specific classification heads\n","        # age branch\n","        self.age_classifier = tf.keras.Sequential([\n","            layers.Dense(128, activation='relu', name='age_dense'),\n","            layers.Dense(num_age_classes, activation='softmax', name='age_output')\n","        ], name='age_head')\n","\n","        # gender branch\n","        self.gender_classifier = tf.keras.Sequential([\n","            layers.Dense(1, activation='sigmoid', name='gender_output')\n","        ], name='gender_head')\n","\n","        # race branch\n","        self.race_classifier = tf.keras.Sequential([\n","            layers.Dense(num_race_classes, activation='softmax', name='race_output')\n","        ], name='race_head')\n","\n","\n","    def call(self, inputs, training=None):\n","        # Get FaceNet embeddings\n","        embeddings = self.backbone(inputs, training=training)\n","\n","        # Shared processing\n","        shared_features = self.shared_dense(embeddings, training=training)\n","        shared_features = self.shared_dropout(shared_features, training=training)\n","\n","        # Task-specific predictions\n","        age_pred = self.age_classifier(shared_features, training=training)\n","        gender_pred = self.gender_classifier(shared_features, training=training)\n","        race_pred = self.race_classifier(shared_features, training=training)\n","\n","        # # Multi-task predictions\n","        # age_pred = self.age_classifier(embeddings, training=training)\n","        # gender_pred = self.gender_classifier(embeddings, training=training)\n","        # race_pred = self.race_classifier(embeddings, training=training)\n","\n","        return {\n","            'age_output': age_pred,\n","            'gender_output': gender_pred,\n","            'race_output': race_pred\n","        }\n","\n","def create_and_compile_model(num_age_classes, num_gender_classes, num_race_classes, freeze_backbone=True):\n","    \"\"\"Create and compile the multi-task model \"\"\"\n","\n","    model = FaceNetMultiTask(\n","        num_age_classes=num_age_classes,\n","        num_gender_classes=num_gender_classes,\n","        num_race_classes=num_race_classes,\n","        freeze_backbone=freeze_backbone\n","    )\n","\n","    # Build the model\n","    model.build((None, 160, 160, 3))\n","\n","    # Optimizer with learning rate schedule\n","    initial_lr = 0.001\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n","        loss={\n","            'age_output': losses.SparseCategoricalCrossentropy(),\n","            'gender_output': tf.keras.losses.BinaryCrossentropy(),\n","            'race_output': losses.SparseCategoricalCrossentropy()\n","        },\n","        loss_weights={\n","            'age_output': 1.0,\n","            'gender_output': 1.0,\n","            'race_output': 1.0\n","        },\n","        metrics={\n","            'age_output': ['sparse_categorical_accuracy'],\n","            'gender_output': ['binary_accuracy'],\n","            'race_output': ['sparse_categorical_accuracy']\n","        }\n","    )\n","\n","    print(f\"\\n Model compiled successfully!\")\n","    print(f\"   - Backbone frozen: {freeze_backbone}\")\n","    print(f\"   - Learning rate: {initial_lr}\")\n","\n","    return model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341269,"status":"ok","timestamp":1754896803647,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"2WTcl-yIykK6","outputId":"fedae64e-41f0-4534-d8c6-547b5ebb1ed3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating training data processor...\n","Removed 0 rows with invalid age groups\n","Sampled training: 50000 samples\n","Classes - Age: 9, Gender: 2, Race: 7\n","Age groups: ['0-2', '10-19', '20-29', '3-9', '30-39', '40-49', '50-59', '60-69', 'more than 70']\n","Gender groups: ['Female', 'Male']\n","Race groups: ['Black', 'East Asian', 'Indian', 'Latino_Hispanic', 'Middle Eastern', 'Southeast Asian', 'White']\n","Original dataset size: 50000\n","Filtered dataset size: 50000\n","\n","Creating validation data processor...\n","Removed 0 rows with invalid age groups\n","Sampled validation: 10000 samples\n","Classes - Age: 9, Gender: 2, Race: 7\n","Original dataset size: 10000\n","Filtered dataset size: 10000\n","\n","Creating datasets...\n"]}],"source":["BATCH_SIZE = 32\n","IMG_SIZE = 160\n","EPOCHS = 20\n","SAMPLE_SIZE = 50000\n","\n","print(\"Creating training data processor...\")\n","train_processor = FairFaceDataProcessor(\n","    TRAIN_CSV_PATH,\n","    DATASET_PATH,\n","    img_size=IMG_SIZE,\n","    sample_size=SAMPLE_SIZE,\n","    is_validation=False\n",").filter_missing_files()\n","\n","print(\"\\nCreating validation data processor...\")\n","val_processor = FairFaceDataProcessor(\n","    VAL_CSV_PATH,\n","    DATASET_PATH,\n","    img_size=IMG_SIZE,\n","    sample_size=SAMPLE_SIZE//5 if SAMPLE_SIZE else None,\n","    is_validation=True\n",").filter_missing_files()\n","\n","# Copy encoders from training to validation processor\n","val_processor.age_encoder = train_processor.age_encoder\n","val_processor.gender_encoder = train_processor.gender_encoder\n","val_processor.race_encoder = train_processor.race_encoder\n","\n","# Re-encode validation labels with training encoders\n","val_processor.df['age_encoded'] = val_processor.age_encoder.transform(val_processor.df['age'])\n","val_processor.df['gender_encoded'] = val_processor.gender_encoder.transform(val_processor.df['gender'])\n","val_processor.df['race_encoded'] = val_processor.race_encoder.transform(val_processor.df['race'])\n","\n","print(\"\\nCreating datasets...\")\n","train_dataset = train_processor.create_dataset(\n","    batch_size=BATCH_SIZE,\n","    augment=True,\n","    shuffle=True\n",")\n","\n","val_dataset = val_processor.create_dataset(\n","    batch_size=BATCH_SIZE,\n","    augment=False,\n","    shuffle=False\n",")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"elapsed":7582,"status":"ok","timestamp":1754896811234,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"bv7OlybkARBz","outputId":"7bb5ce51-779d-49f7-cde1-dd7c832d04b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building model...\n","\n"," Model compiled successfully!\n","   - Backbone frozen: True\n","   - Learning rate: 0.001\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003eModel: \"face_net_multi_task\"\u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1mModel: \"face_net_multi_task\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u003cspan style=\"font-weight: bold\"\u003e Layer (type)                    \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e Output Shape           \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e       Param # \u003c/span\u003e┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ inception_resnet_v1             │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e512\u003c/span\u003e)            │    \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e23,497,424\u003c/span\u003e │\n","│ (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eFunctional\u003c/span\u003e)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ shared_dense (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)            │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ shared_dropout (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDropout\u003c/span\u003e)        │ ?                      │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ age_head (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eSequential\u003c/span\u003e)           │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gender_head (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eSequential\u003c/span\u003e)        │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ race_head (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eSequential\u003c/span\u003e)          │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","\u003c/pre\u003e\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ inception_resnet_v1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m23,497,424\u001b[0m │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ shared_dense (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ shared_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ age_head (\u001b[38;5;33mSequential\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gender_head (\u001b[38;5;33mSequential\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ race_head (\u001b[38;5;33mSequential\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Total params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e23,497,424\u003c/span\u003e (89.64 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,497,424\u001b[0m (89.64 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Non-trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e23,497,424\u003c/span\u003e (89.64 MB)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,497,424\u001b[0m (89.64 MB)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["print(\"Building model...\")\n","model = create_and_compile_model(\n","    num_age_classes=train_processor.num_classes['age'],\n","    num_gender_classes=train_processor.num_classes['gender'],\n","    num_race_classes=train_processor.num_classes['race'],\n","    freeze_backbone=True  # Freeze FaceNet weights initially\n",")\n","\n","print(model.summary())"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1754897108153,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"MWotdjMtbOOw"},"outputs":[],"source":["callbacks = [\n","    tf.keras.callbacks.ModelCheckpoint(\n","        '/content/best_fairface_facenet_model.h5',\n","        monitor='val_loss',\n","        save_best_only=True,\n","        save_weights_only=False,\n","        verbose=1\n","    ),\n","    tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor='val_loss',\n","        factor=0.5,\n","        patience=5,\n","        min_lr=1e-7,\n","        verbose=1\n","    ),\n","    tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        patience=10,\n","        restore_best_weights=True,\n","        verbose=1\n","    ),\n","    tf.keras.callbacks.CSVLogger('training_log.csv')\n","]\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6783,"status":"ok","timestamp":1754897125105,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"9ys2IgG9HiWZ","outputId":"035fdfe4-d78b-4cdb-b4af-7c53ea2381fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== Dataset Structure Verification ===\n","Training CSV shape: (86744, 5)\n","Validation CSV shape: (10954, 5)\n","Training CSV columns: ['file', 'age', 'gender', 'race', 'service_test']\n","Training images found: 86744\n","Validation images found: 10974\n","\n","=== File Matching Check ===\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/1.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/2.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/3.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/4.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/5.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/6.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/7.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/8.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/9.jpg\n","✓ Found: /content/drive/MyDrive/datasets/FairFace/train/10.jpg\n","✓ All sampled files found!\n","\n","First few rows of training CSV:\n","          file    age  gender        race  service_test\n","0  train/1.jpg  50-59    Male  East Asian          True\n","1  train/2.jpg  30-39  Female      Indian         False\n","2  train/3.jpg    3-9  Female       Black         False\n","3  train/4.jpg  20-29  Female      Indian          True\n","4  train/5.jpg  20-29  Female      Indian          True\n","\n","✓ Dataset structure looks good! Proceeding with training...\n"]}],"source":["def verify_dataset_structure():\n","    print(\"=== Dataset Structure Verification ===\")\n","\n","    # Check CSV files\n","    train_csv = pd.read_csv(TRAIN_CSV_PATH)\n","    val_csv = pd.read_csv(VAL_CSV_PATH)\n","\n","    print(f\"Training CSV shape: {train_csv.shape}\")\n","    print(f\"Validation CSV shape: {val_csv.shape}\")\n","    print(f\"Training CSV columns: {list(train_csv.columns)}\")\n","\n","    # Check image directories\n","    train_images = os.listdir(TRAIN_IMG_DIR)\n","    val_images = os.listdir(VAL_IMG_DIR) if os.path.exists(VAL_IMG_DIR) else []\n","\n","    print(f\"Training images found: {len(train_images)}\")\n","    print(f\"Validation images found: {len(val_images)}\")\n","\n","    # Check if CSV files match image files\n","    print(\"\\n=== File Matching Check ===\")\n","\n","    # Sample a few files from CSV and check if they exist\n","    sample_files = train_csv['file'].head(10).tolist()\n","    missing_files = []\n","\n","    for file_name in sample_files:\n","        # full_path = os.path.join(TRAIN_IMG_DIR, file_name)\n","        full_path = os.path.join(DATASET_PATH, file_name)\n","        if not os.path.exists(full_path):\n","            missing_files.append(full_path)\n","        else:\n","            print(f\"✓ Found: {full_path}\")\n","\n","    if missing_files:\n","        print(f\"✗ Missing files: {missing_files}\")\n","    else:\n","        print(\"✓ All sampled files found!\")\n","\n","    # Check first few rows of CSV\n","    print(f\"\\nFirst few rows of training CSV:\")\n","    print(train_csv.head())\n","\n","    return len(missing_files) == 0\n","\n","# Run verification\n","dataset_ok = verify_dataset_structure()\n","\n","if dataset_ok:\n","    print(\"\\n✓ Dataset structure looks good! Proceeding with training...\")\n","else:\n","    print(\"\\n✗ Dataset structure issues found. Please check file paths.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"19fY68tkbhW6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training...\n","Epoch 1/10\n","   1563/Unknown \u001b[1m20554s\u001b[0m 13s/step - age_output_loss: 1.4492 - age_output_sparse_categorical_accuracy: 0.4203 - gender_output_binary_accuracy: 0.8290 - gender_output_loss: 0.4034 - loss: 3.1451 - race_output_loss: 1.2926 - race_output_sparse_categorical_accuracy: 0.5278\n","Epoch 1: val_loss improved from inf to 2.50303, saving model to /content/best_fairface_facenet_model.h5\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24611s\u001b[0m 15s/step - age_output_loss: 1.4491 - age_output_sparse_categorical_accuracy: 0.4203 - gender_output_binary_accuracy: 0.8291 - gender_output_loss: 0.4033 - loss: 3.1450 - race_output_loss: 1.2925 - race_output_sparse_categorical_accuracy: 0.5278 - val_age_output_loss: 1.1680 - val_age_output_sparse_categorical_accuracy: 0.5109 - val_gender_output_binary_accuracy: 0.8700 - val_gender_output_loss: 0.2950 - val_loss: 2.5030 - val_race_output_loss: 1.0321 - val_race_output_sparse_categorical_accuracy: 0.6047 - learning_rate: 0.0010\n","Epoch 2/10\n","\u001b[1m1095/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m16:57\u001b[0m 2s/step - age_output_loss: 1.2426 - age_output_sparse_categorical_accuracy: 0.4853 - gender_output_binary_accuracy: 0.8688 - gender_output_loss: 0.3006 - loss: 2.6572 - race_output_loss: 1.1141 - race_output_sparse_categorical_accuracy: 0.5788"]}],"source":["print(\"Starting training...\")\n","history = model.fit(\n","    train_dataset,\n","    epochs=10,\n","    validation_data=val_dataset,\n","    callbacks=callbacks,\n","    verbose=1\n",")\n","\n","print(\"Training completed!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwNWKg_bW2Gg"},"outputs":[],"source":["print(\"Available history keys:\", list(history.history.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfeTgSyaEQxk"},"outputs":[],"source":["\n","def plot_history(history):\n","    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n","\n","    # Total Loss\n","    ax1.plot(history.history['loss'], label='Total Loss', color='blue')\n","    ax1.set_title('Total Training Loss')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.legend()\n","    ax1.grid(True)\n","\n","    # Individual Task Losses\n","    ax2.plot(history.history['age_output_loss'], label='Age Loss', color='red')\n","    ax2.plot(history.history['gender_output_loss'], label='Gender Loss', color='green')\n","    ax2.plot(history.history['race_output_loss'], label='Race Loss', color='orange')\n","    ax2.set_title('Individual Task Losses')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Loss')\n","    ax2.legend()\n","    ax2.grid(True)\n","\n","    # Task Accuracies\n","    ax3.plot(history.history['age_output_sparse_categorical_accuracy'], label='Age Accuracy', color='red')\n","    ax3.plot(history.history['gender_output_sparse_categorical_accuracy'], label='Gender Accuracy', color='green')\n","    ax3.plot(history.history['race_output_sparse_categorical_accuracy'], label='Race Accuracy', color='orange')\n","    ax3.set_title('Task Accuracies')\n","    ax3.set_xlabel('Epoch')\n","    ax3.set_ylabel('Accuracy')\n","    ax3.legend()\n","    ax3.grid(True)\n","\n","    # Learning Rate\n","    ax4.plot(history.history['learning_rate'], label='Learning Rate', color='purple')\n","    ax4.set_title('Learning Rate Schedule')\n","    ax4.set_xlabel('Epoch')\n","    ax4.set_ylabel('Learning Rate')\n","    ax4.legend()\n","    ax4.grid(True)\n","    ax4.set_yscale('log')  # Log scale for learning rate\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Print final metrics\n","    print(\"Final Training Metrics:\")\n","    print(f\"Total Loss: {history.history['loss'][-1]:.4f}\")\n","    print(f\"Age Loss: {history.history['age_output_loss'][-1]:.4f}\")\n","    print(f\"Gender Loss: {history.history['gender_output_loss'][-1]:.4f}\")\n","    print(f\"Race Loss: {history.history['race_output_loss'][-1]:.4f}\")\n","    print(f\"Age Accuracy: {history.history['age_output_sparse_categorical_accuracy'][-1]:.4f}\")\n","    print(f\"Gender Accuracy: {history.history['gender_output_sparse_categorical_accuracy'][-1]:.4f}\")\n","    print(f\"Race Accuracy: {history.history['race_output_sparse_categorical_accuracy'][-1]:.4f}\")\n","    print(f\"Final Learning Rate: {history.history['learning_rate'][-1]:.2e}\")\n","\n","plot_history(history)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5bCJhEpWFdK"},"outputs":[],"source":["def predict_sample(model, img_path, train_processor, img_size=(160, 160)):\n","    \"\"\"\n","    Predict demographics for a single image\n","    \"\"\"\n","    # Load and preprocess image - matching training preprocessing\n","    img = tf.io.read_file(img_path)\n","    img = tf.image.decode_image(img, channels=3)\n","    img = tf.image.resize(img, img_size)\n","    img = tf.cast(img, tf.float32) / 255.0\n","\n","    # Apply same normalization as training (ImageNet standardization)\n","    img = tf.image.per_image_standardization(img)\n","    img = tf.expand_dims(img, axis=0)  # Add batch dim\n","\n","    # Predict\n","    preds = model.predict(img, verbose=0)\n","\n","    # Extract predictions - model returns a dictionary\n","    age_pred = np.argmax(preds['age_output'][0])\n","    gender_pred = np.argmax(preds['gender_output'][0])\n","    race_pred = np.argmax(preds['race_output'][0])\n","\n","    # Decode labels using the training processor encoders\n","    age_label = train_processor.age_encoder.inverse_transform([age_pred])[0]\n","    gender_label = train_processor.gender_encoder.inverse_transform([gender_pred])[0]\n","    race_label = train_processor.race_encoder.inverse_transform([race_pred])[0]\n","\n","    # Display - load original image for display\n","    display_img = tf.io.read_file(img_path)\n","    display_img = tf.image.decode_image(display_img, channels=3)\n","    display_img = tf.image.resize(display_img, img_size)\n","    display_img = tf.cast(display_img, tf.float32) / 255.0\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(display_img)\n","    plt.title(f\"Predictions:\\nAge: {age_label}\\nGender: {gender_label}\\nRace: {race_label}\")\n","    plt.axis('off')\n","    plt.show()\n","\n","    return age_label, gender_label, race_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnTpCSV1WdZC"},"outputs":[],"source":["def test_predictions(model, train_processor, num_samples=3):\n","    \"\"\"Test predictions on random samples from filtered training data\"\"\"\n","\n","    # Get some sample files from the filtered training data\n","    sample_indices = np.random.choice(len(train_processor.df), num_samples, replace=False)\n","\n","    for i, idx in enumerate(sample_indices):\n","        print(f\"\\n=== Sample {i+1} ===\")\n","        row = train_processor.df.iloc[idx]\n","        img_path = os.path.join(train_processor.img_dir, row['file'])\n","\n","        # True labels\n","        true_age = row['age']\n","        true_gender = row['gender']\n","        true_race = row['race']\n","\n","        print(f\"True labels - Age: {true_age}, Gender: {true_gender}, Race: {true_race}\")\n","\n","        # Predict\n","        pred_age, pred_gender, pred_race = predict_sample(model, img_path, train_processor)\n","\n","        # Compare\n","        print(f\"Predicted - Age: {pred_age}, Gender: {pred_gender}, Race: {pred_race}\")\n","        print(f\"Correct - Age: {true_age == pred_age}, Gender: {true_gender == pred_gender}, Race: {true_race == pred_race}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"admPSvGGWOPj"},"outputs":[],"source":["test_predictions(model, train_processor, num_samples=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":172946,"status":"aborted","timestamp":1754895966752,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"BEU-ZGsXWRoh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":172948,"status":"aborted","timestamp":1754895966754,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"4J2u7vOMWJaF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":172949,"status":"aborted","timestamp":1754895966756,"user":{"displayName":"Ayobami Adeyemo","userId":"07870659355035613187"},"user_tz":-60},"id":"LOvD716QWgSZ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM5nJ9pyH/KBim4w591wgqE","gpuType":"T4","machine_shape":"hm","mount_file_id":"1QWnY7-M1agv-s49cd3xZqMZqsRkQlEls","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}